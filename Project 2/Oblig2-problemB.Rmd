---
title: "Oblig2-TMA4300"
author: "Martine Middelthon"
date: "27 2 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem B

```{r}
# Read and plot data
gaussiandata = read.delim("gaussiandata.txt")
y = gaussiandata[,1]
t = seq(from=1,to=length(y),by=1)
plot(t,y)
```

## 1.
We consider the problem of smoothing the time series that is plotted above. We assume that given the vector of linear predictors $\boldsymbol\eta=(\eta_1,\ldots,\eta_T)$, where in this case $T=20$, the observations $y_t$ are independent and distributed according to
$$
  y_t \mid \eta_t \sim \mathcal{N}(\eta_t,1)\quad,
$$
for $t=1,\ldots,T$.
The linear predictor for time $t$ is $\eta_t = f_t$, where $f_t$ is the smooth effect for time $t$. For the prior distribution of $\mathbf{f}=(f_1,\ldots,f_T)$ we have a second order random walk model, that is,
$$
  \pi(\mathbf{f}\mid\theta) \propto \theta^{(T-2)/2} \text{exp}\Big\{-\frac{\theta}{2} \sum_{t=3}^T (f_t-2f_{t-1}+f_{t-2}^2) \Big\}=\mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}) \quad,
$$
where $\mathbf{Q}$ is the precision matrix and $\theta$ is the precision parameter that controls the smoothness of $\mathbf{f}$. We assume that the $Gamma(1,1)$-distribution is the prior for $\theta$.

The model described here can be written as the hierarchichal model:
$$
  \begin{aligned}
    \mathbf{y}\mid\mathbf{f} &\sim \prod_{t=1}^T P(y_t\mid \eta_t) \\
    \mathbf{f}\mid\theta &\sim \pi(\mathbf{f}\mid\theta) = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}) \\
    \theta &\sim Gamma(1,1)
  \end{aligned}
$$
Here, the first line is the likelihood of the response $\mathbf{y}=(y_1,\ldots,y_T)$, the second line gives the prior distribution of the latent field, and the third line gives the prior distribution of the hyperparameter $\theta$. Since our model has this particular structure, it is a latent Gaussian model.
INLA can be used to estimate the parameters because we have a latent gaussian model where each data point $y_t$ depends only on the one element $f_t$ in the latent field, the dimension of the hyperparameter is one and the precision matrix $\mathbf{Q}(\theta)$ of the latent field is sparse.

## 2.

Here, we implement a block Gibbs sampling algorithm for $f(\boldsymbol\eta,\theta\mid \mathbf{y})$, where we propose a new value for $\theta$ from the full conditional $\pi(\theta\mid\boldsymbol\eta,\mathbf{y})$ and a new value for $\boldsymbol\eta$ from the full conditional $\pi(\boldsymbol\eta\mid\theta,\mathbf{y})$. Thus, we need to find these distributions.
We start with the posterior
$$
  \pi(\boldsymbol\eta,\theta\mid\mathbf{y}) \propto \pi(\theta) \pi(\boldsymbol\eta\mid\theta) \prod_{t=1}^T\pi(y_t\mid\eta_t,\theta) \propto \frac{\theta^{(T-2)/2}}{(2\pi)^{T/2}} \exp\bigg\{-\theta -\frac{\theta}{2}\sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 -\frac{1}{2}\sum_{t=1}^T(y_t-\eta_t)^2 \bigg\}.
$$
Then we find the full conditional for $\theta$ to be
$$
  \begin{aligned}
    \pi(\theta\mid\mathbf{y},\boldsymbol\eta) &\propto \theta^{T/2-1} \exp\bigg\{-\theta\bigg(1+\frac{1}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2\bigg) \bigg\} \\
    &\propto \textit{Gamma}\bigg(\frac{T}{2}, 1+\frac{1}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 \bigg)
  \end{aligned}.
$$
The full conditional for $\boldsymbol\eta$ is
$$
  \begin{aligned}
    \pi(\boldsymbol\eta\mid\theta,\mathbf{y}) &\propto \exp\bigg\{-\frac{\theta}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 -\frac{1}{2}\sum_{t=1}^T(y_t-\eta_t)^2 \bigg\} \\
    &= \exp\bigg\{-\frac{1}{2}\bigg(\boldsymbol\eta^T\mathbf{Q}\boldsymbol\eta+ (\mathbf{y}-\boldsymbol\eta)^T(\mathbf{y}-\boldsymbol\eta)  \bigg)\bigg\}\\
    &= \exp\bigg\{-\frac{1}{2}\boldsymbol\eta^T(\mathbf{Q}+\mathbf{I})\boldsymbol\eta+\mathbf{y}^T\boldsymbol\eta \bigg\}
  \end{aligned}.
$$
Here, $\mathbf{Q}(\theta)=\theta \mathbf{L}\mathbf{L}^T$ is the precision matrix, where $\mathbf{L}$ is the $T\times (T-2)$ matrix
$$
  \mathbf{L}=
  \begin{bmatrix}
    1 & -2 & 1 & 0 & 0 & 0 & \ldots \\
    0 & 1 & -2 & 1 & 0 &0&\ldots\\
    \vdots & & \ddots &\ddots & \ddots \\
    0 & 0 & 0 & 0 & 1 & -2 & 1 \\
    0 & 0 & 0 & 0 & 0 & 1 & -2 \\
    0 & 0 & 0 & 0 & 0 & 0 & 1
  \end{bmatrix}.
$$
By looking at the last line in the above expression for $\pi(\boldsymbol\eta\mid\theta,\mathbf{y})$, we recognize that the canonical parametrization is $\mathcal{N}(\mathbf{y}, \mathbf{Q}+\mathbf{I})$, and find that $\pi(\boldsymbol\eta\mid\theta,\mathbf{y}) \propto \mathcal{N}((\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y},(\mathbf{Q}+\mathbf{I})^{-1})$.
In the algorithm we sample the new proposals for the parameters from these two distributions that we have found for the full conditionals. We always use the last updated parameters.

```{r}
library(Matrix)
library(mvtnorm)

# Function to make the precision matrix
make.Q = function(T, theta) {
  # Make the matrix L as described in the text
  L = diag(T)
  d1 = rep(-2,T-1)
  d2 = rep(1, T-2)
  L[row(L)-col(L)==1] = d1
  L[row(L)-col(L)==2] = d2
  L = L[,-c(T-1,T)]
  # Compute Q(theta)
  Q = theta * L %*% t(L)
  return(Q)
}

# Function for block Gibbs sampling
# n is the number of samples including the inital value
sample.Gibbs = function(n, theta.init, f.init, y) {
  T = length(f.init)
  # Make vector and matrix for storing the samples
  theta.vec = rep(0,n)
  f.matrix = matrix(1:T*n, nrow = T, ncol = n)
  # Initialize 
  theta.vec[1] = theta.init
  f.matrix[, 1] = f.init
  # Iterations
  for(i in 2:n) {
    # Sample theta
    summ = 0
    for(t in 3:T) {
      summ = summ + (f.matrix[t, i-1] - 2*f.matrix[t-1, i-1] + f.matrix[t-2, i-1])^2
    }
    theta.vec[i] = rgamma(1, shape = T/2, rate = 1 + 0.5*summ)
    # Sample f
    Q = make.Q(T, theta.vec[i])
    f.mean = solve(Q+diag(T)) %*% y
    f.sigma = solve(Q+diag(T))
    f.matrix[, i] = rmvnorm(1, f.mean, f.sigma)
  }
  return(rbind(f.matrix, theta.vec))    # Return concatenated matrix with f and theta samples
}

# Set values
n = 10000
T = length(y)
theta.init = 1
f.init = rep(2,T)
# Sample
result = sample.Gibbs(n, theta.init, f.init, y)
result.theta = result[length(result[,1]), -1]   # Extracting the theta samples, excluding the initial value
result.f = result[-length(result[,1]), -1]      # Extracting the f samples, excluding the initial values

# Estimate for the posterior marginal for theta
hist(result.theta, xlab = "Theta", main = "Histogram of theta samples")

# Vectors for storing the mean, variance and confidence bounds
f.mean = rep(0,T)
f.var = rep(0,T)
conf.upper = rep(0,T)
conf.lower = rep(0,T)
# Calculate the mean and variance
for(t in 1:T) {
  f.mean[t] = mean(result.f[t,])
  f.var[t] = var(result.f[t,])
}
# Calculate 95% confidence bounds
for(t in 1:T) {
  z = qnorm(0.025)
  conf.upper[t] = f.mean[t] + z * sqrt(f.var[t]) 
  conf.lower[t] = f.mean[t] - z * sqrt(f.var[t]) 
}
# Plotting
t = seq(from = 1, to = T, by = 1)
plot(t, f.mean, type = "l", ylab = "y")
points(t, y)                      
lines(t, conf.lower, col = "red")
lines(t, conf.upper, col = "red")
```
The histogram shows an estimate for $\pi(\theta\mid\mathbf{y})$.
In the plot the data points are plotted as circles. The black line is plotted using the estimates of the smooth effects. The red lines are the $95\%$ confidence bounds. Almost all the data points are within the bounds.

## 3.

```{r}

pi_theta_y = function(theta.grid, y) {
  pi = rep(0,length(theta.grid))
  T = 20
  for(i in 1:length(pi)){
    theta = theta.grid[i]
    Q = make.Q(T, theta)
    deter = det(solve(Q+diag(T)))
    pi[i] = theta^(T/2-1) * exp(-theta) * deter^(0.5) * exp(-0.5 * t(y) %*% y)
  }
  return(pi)
}

thetas = seq(from = 0, to = 3, by = 0.1)
pi = pi_theta_y(thetas, y)
plot(thetas, pi)
```


## 5.

```{r}
library(INLA)
T = 20
t = seq(from = 1, to = T, by = 1)
data = list(y = y, t = t)

thetaprior = list(prior = "gamma", param = c(1, 1))
formula = y âˆ¼ f(t, model = "rw2", hyper = list(prec = thetaprior), constr = FALSE) - 1
result = INLA::inla(formula = formula, family = "gaussian", data = data)

```

