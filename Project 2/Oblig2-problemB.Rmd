---
title: "Oblig2-TMA4300"
author: "Martine Middelthon"
date: "27 2 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem B

```{r}
# Read and plot data
gaussiandata = read.delim("gaussiandata.txt")
y = gaussiandata[,1]
t = seq(from=1,to=length(y),by=1)
plot(t,y)
```

## 1.
We consider the problem of smoothing the time series that is plotted above. We assume that given the vector of linear predictors $\boldsymbol\eta=(\eta_1,\ldots,\eta_T)$, where in this case $T=20$, the observations $y_t$ are independent and distributed according to
$$
  y_t \mid \eta_t \sim \mathcal{N}(\eta_t,1)\quad,
$$
for $t=1,\ldots,T$.
The linear predictor for time $t$ is $\eta_t = f_t$, where $f_t$ is the smooth effect for time $t$. For the prior distribution of $\mathbf{f}=(f_1,\ldots,f_T)$ we have a second order random walk model, that is,
$$
  \pi(\mathbf{f}\mid\theta) \propto \theta^{(T-2)/2} \text{exp}\Big\{-\frac{\theta}{2} \sum_{t=3}^T (f_t-2f_{t-1}+f_{t-2}^2) \Big\}=\mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}) \quad,
$$
where $\mathbf{Q}$ is the precision matrix and $\theta$ is the precision parameter that controls the smoothness of $\mathbf{f}$. We assume that the $Gamma(1,1)$-distribution is the prior for $\theta$.

The model described here can be written as the hierarchichal model:
$$
  \begin{aligned}
    \mathbf{y}\mid\mathbf{f} &\sim \prod_{t=1}^T P(y_t\mid \eta_t) \\
    \mathbf{f}\mid\theta &\sim \pi(\mathbf{f}\mid\theta) = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}) \\
    \theta &\sim Gamma(1,1)
  \end{aligned}
$$
Here, the first line is the likelihood of the response $\mathbf{y}=(y_1,\ldots,y_T)$, the second line gives the prior distribution of the latent field, and the third line gives the prior distribution of the hyperparameter $\theta$. Since our model has this particular structure, it is a latent Gaussian model.
Q is sparse
theta has small dimension
every observation/data point depends only on one eta/f
have latent gaussian model

## 2.

```{r}
library(Matrix)
library(mvtnorm)

make.Q = function(T, theta) {
  #k = c(0, -1, -2)
  #diags = list(rep(1,T), rep(-2,T-1), rep(1,T-2))
  #L = bandSparse(T,T,k,diagonals = diags)
  L = diag(T)
  d1 = rep(-2,T-1)
  d2 = rep(1, T-2)
  L[row(L)-col(L)==1] = d1
  L[row(L)-col(L)==2] = d2
  M = L[,-c(T-1,T)]
  Q = theta * M %*% t(M)
  return(Q)
}

sample.Gibbs = function(n, theta.init, f.init, y) {
  T = length(f.init)
  theta.vec = rep(0,n)
  f.matrix = matrix(1:T*n, nrow = T, ncol = n)
  # Initialize
  theta.vec[1] = theta.init
  f.matrix[, 1] = f.init
  # Iterations
  for(i in 2:n) {
    # Theta
    summ = 0
    for(t in 3:T) {
      summ = summ + (f.matrix[t, i-1] - 2*f.matrix[t-1, i-1] + f.matrix[t-2, i-1])^2
    }
    theta.vec[i] = rgamma(1, shape = T/2, rate = 1 + 0.5*summ)
    # f
    Q = make.Q(T, theta.vec[i])
    f.mean = solve(Q+diag(T)) %*% y
    f.sigma = solve(Q+diag(T))
    f.matrix[, i] = rmvnorm(1, f.mean, f.sigma)
  }
  return(rbind(f.matrix, theta.vec))
}

n = 10000
T = length(y)
theta.init = 1
f.init = rep(2,T)
result = sample.Gibbs(n, theta.init, f.init, y)
result.theta = result[length(result[,1]), -1]
result.f = result[-length(result[,1]), -1]

hist(result.theta)


f.mean = rep(0,T)
f.var = rep(0,T)
conf.upper = rep(0,T)
conf.lower = rep(0,T)

for(t in 1:T) {
  f.mean[t] = mean(result.f[t,])
  f.var[t] = var(result.f[t,])
}

for(t in 1:T) {
  z = qnorm(0.025)
  conf.upper[t] = f.mean[t] + z * sqrt(f.var[t]) 
  conf.lower[t] = f.mean[t] - z * sqrt(f.var[t]) 
}

t = seq(from = 1, to = T, by = 1)
plot(t, f.mean, type = "l")
points(t, y)
lines(t, conf.lower, col = "red")
lines(t, conf.upper, col = "red")


```

## 3.

```{r}

pi_theta_y = function(theta.grid, y) {
  pi = rep(0,length(theta.grid))
  T = 20
  for(i in 1:length(pi)){
    theta = theta.grid[i]
    Q = make.Q(T, theta)
    deter = det(solve(Q+diag(T)))
    pi[i] = theta^(T/2-1) * exp(-theta) * deter^(0.5) * exp(-0.5 * t(y) %*% y)
  }
  return(pi)
}

thetas = seq(from = 0, to = 3, by = 0.1)
pi = pi_theta_y(thetas, y)
plot(thetas, pi)
```

