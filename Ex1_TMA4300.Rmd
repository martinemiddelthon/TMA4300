---
title: "Ex1 - TMA4300"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A

## 1.
```{r}
# Function to sample n times from exponential distribution
samples_exp = function(rate, n){
  u = runif(n)          # Sample from unif(0,1) distribution
  x = - log(u) / rate   # Compute values using inverse cdf
  return(x)             # Vector of n samples
}

# Function to print mean and variance
print.mean.var = function(n, m.emp, m.th, v.emp, v.th) {
  cat('Number of samples:', n)
  cat('\n\nEmpirical mean:', m.emp)
  cat('\nTheoretical mean:', m.th)
  cat('\n\nEmpirical variance:', v.emp)
  cat('\nTheoretical variance:', v.th)
}

# Check function
rate = 4                                        # Set rate parameter
n = 10000                                       # Number of samples
exp.sample = samples_exp(rate, n)               # Sample
hist(exp.sample)                                # Histogram of sample

mean.emp.exp = mean(exp.sample)                 # Empirical mean
mean.th.exp = 1 / rate                          # Theoretical mean

var.emp.exp = var(exp.sample)                   # Empirical variance
var.th.exp = 1 / rate^2                         # Theoretical variance

print.mean.var(n, mean.emp.exp, mean.th.exp, var.emp.exp, var.th.exp)  # Print values
```

## 2.

** a)
We consider the probability density function
$$
  g(x) = \begin{cases} 0, & x\leq 0 \\ cx^{\alpha -1}, & 0 < x < 1 \\ ce^{-x}, & x \geq 1 \end{cases},
$$
where $\alpha\in (0,1)$ and $c$ is a normalizing constant. Given $\alpha$, we find that the normalizing constant is $c=\frac{\alpha e}{\alpha + e}$.
By integrating $g(x)$ from $-\infty$ to $x$, the cumulative distribution function is found to be
$$
  F(x) = P(X\leq x) = \begin{cases} 0, & x\leq 0 \\ \frac{c}{\alpha}x^{\alpha}, & 0 < x < 1 \\ c(\frac{1}{\alpha}+\frac{1}{e}-e^{-x}), & x \geq 1 \end{cases}.
$$
In order to find the inverse cdf we let $F(x)=y$. The cdf maps $x\in(-\infty,\infty)$ to $y\in[0,1]$. By solving $F(x)=y$ with respect to $x$, we find that the inverse of the cdf is
$$
  F^{-1}(y) = \begin{cases} (\frac{\alpha y}{c})^{1/\alpha}, & 0 \leq y \leq \frac{c}{\alpha} \\ -\ln(\frac{1}{\alpha}+\frac{1}{e}-\frac{y}{c}), & y > \frac{c}{\alpha} \end{cases}.
$$


** b)

```{r}
# Function to sample from g using inverse cdf
samples_g = function(alpha, n) {
  c = alpha * exp(1) / (alpha + exp(1))   # Normalizing constant
  u = runif(n)                            # Sample from unif(0,1) distribution
  vec = rep(0, n)                         # Vector to store the generated samples
  for(i in 1:length(u)){
    if(u[i]<= c/alpha) {vec[i] = (alpha * u[i] / c)^(1 / alpha)}  # Use case 1 in inverse cdf to generate sample
    else {vec[i] = -log(1/alpha + exp(-1) - u[i]/c)}              # Use case 2 in inverse cdf to generate sample
  }
  return(vec)
}

# Check function
alpha = 0.8                               # Set parameter
c = alpha * exp(1) / (alpha + exp(1))     # Normalizing constant
n = 10000                                 # Number of samples
samples.g = samples_g(alpha, n)           # Sampling from g
hist(samples.g)                           # Histogram of samples

mean.emp.g = mean(samples.g)                      # Empirical mean
mean.theo.g = c * (1/(alpha+1) + 2*exp(-1))       # Theoretical mean

var.emp.g = var(samples.g)                                  # Empirical variance
var.theo.g = c * (1/(alpha+2) + 5*exp(-1)) - mean.theo.g^2  # Theoretical variance

print.mean.var(n, mean.emp.g, mean.theo.g, var.emp.g, var.theo.g)   # Print values
```

## 3.

```{r}
# Function to generate n samples from N(0,1) using Box Muller
boxmuller = function(n) {
  # Sample from unif(0,1)
  u1 = runif(n)
  u2 = runif(n)
  # Polar coordinates
  r = sqrt(-2*log(u1))
  theta = 2*pi*u2
  #Cartesian coordinate(s)
  x = r*cos(theta)          # Vector of n independent samples
  #y = r*sin(theta)
  return(x)
}

# Check function
n = 10000                       # Number of samples
bm.samples = boxmuller(n)       # Sampling from N(0,1)
hist(bm.samples)                # Histogram of samples

mean_bm = mean(bm.samples)      # Empirical mean
var_bm = var(bm.samples)        # Empirical variance

print.mean.var(n, mean_bm, 0, var_bm, 1)  # Print values
```


## 4.

```{r}
# Function to generate a realization from d-variate normal distribution
mult_normal = function(d, mu, sigma) {
  # mu is vector of means, must have length d
  # sigma is covariance matrix, must have dimension dxd
  x = boxmuller(d)    # d N(0,1) distributed variables
  A = t(chol(sigma))  # Cholesky decomposition of covariance matrix (lower triangular)
  y = mu + A %*% x    # d-variate N(mu, sigma)
  return(y)
}


# Check function

# vector of means and covariance matrix
mu = c(1,2)         
sigma = cbind(c(1,0.5),c(0.5,1))

# Estimate mean and empirical variance
n = 10000                     # Number of realizations
mu_est = rep(0,length(mu))    
multnor.sample = matrix(NA, n, length(mu))
for(i in 1:n) {
  multnor = mult_normal(length(mu),mu,sigma)
  mu_est = mu_est + multnor
  multnor.sample[i,] = multnor
  }
mu_est = mu_est / n               # Estimated mean vector

sigma_est = var(multnor.sample)   # Estimated covariance matrix

# Printing
cat('True mean:', mu)
cat('\nEstimated mean:', mu_est)
cat('\n\nTrue covariance matrix:\n')
sigma
cat('\nEstimated covariance matrix:\n')
sigma_est
```


# Problem B
## 1.
** a)

We consider a gamma distribution with parameters $\alpha\in (0,1)$ and $\beta=1$. It has probability density function
$$
  f(x) = \begin{cases} \frac{1}{\Gamma(\alpha)} x^{\alpha-1}e^{-x}, & x>0 \\ 0, & \text{otherwise} \end{cases}.
$$
We want to use rejection sampling to generate samples from $f$ by proposing samples from $g$. Thus we need the acceptance probability $A(x) = \frac{1}{k}\frac{f(x)}{g(x)}$, so we have to find a $k>1$ such that $\frac{f(x)}{g(x)}\leq k$ for all $x$ where $f(x)>0$. We evaluate this fraction when $0<x<1$ and find that
$$
  \frac{f(x)}{g(x)} = \frac{e^{-x}}{c\Gamma(\alpha)} \leq \frac{1}{c\Gamma(\alpha)},
$$
on this interval. Here, $c$ is the normalizing constant in $g$. When $x\geq 1$ the fraction becomes
$$
  \frac{f(x)}{g(x)} = \frac{x^{\alpha-1}}{c\Gamma(\alpha)} \leq \frac{1}{c\Gamma(\alpha)}.
$$
Thus, we set $k=\frac{1}{c\Gamma(\alpha)}$, and the acceptance probability becomes
$$
  A(x) = \begin{cases} e^{-x}, & 0<x<1 \\ x^{\alpha-1}, & x\geq 1 \end{cases}.
$$

** b)

```{r}
# Function to calculate the acceptance probability
accprob = function(x, alpha) {
  if(x<1) {return(exp(-x))}
  else if(x>=1) {return(x^(alpha-1))}
}

# Function to generate n samples from f
gamma_rejsamp = function(n, alpha) {
  vec = rep(0, n)             # Vector for storing the accepted samples
  i = 1
  while(i<=n) {
    x = samples_g(alpha, 1)   # Sample from proposal density
    u = runif(1)              # Sample from unif(0,1)
    a = accprob(x, alpha)     # Calculate acceptance probability
    if(u<=a) {                # Accept x as a sample
      vec[i] = x              # Store x
      i = i + 1
      }
  }
  return(vec)
}

# Check function
alpha = 0.5     # Set parameter
n = 10000       # Number of samples
gamma.sample = gamma_rejsamp(n, alpha)    # Generate samples

mean.emp = mean(gamma.sample)   # Empirical mean
mean.theo = alpha               # Theoretical mean

var.emp = var(gamma.sample)     # Empirical variance
var.theo = alpha                # Theoretical variance

print.mean.var(n, mean.emp, mean.theo, var.emp, var.theo)   # Printing values
```


## 2.

** a)

We consider a gamma distribution with parameters $\alpha>1$ and $\beta=1$, and we want to use the ratio of uniforms method to generate samples from this distribution. We define $C_f = \{(x_1,x_2):0\leq x_1\leq \sqrt{f^*(\frac{x_1}{x_2})}\}$ and
$$
  f^*(x) = \begin{cases} x^{\alpha-1}e^{-x}, & x>0 \\ 0, & \text{otherwise} \end{cases}.
$$
We also define $a = \sqrt{\sup_x f^*(x)}$, $b_- = -\sqrt{\sup_{x\leq0}(x^2f^*(x))}$ and $b_+ = \sqrt{\sup_{x\geq0}(x^2f^*(x))}$, so that $C_f\subset[0,a]\times[b_-,b_+]$.
Since $f^*(x)$ is bounded and positive for $x>0$, we find its maximum by differentiating it on this interval and setting it equal to zero. This gives $x=0$ and $x=\alpha-1$, where the latter clearly gives the maximum. Thus we find that
$$
  a = \sqrt{f^*(\alpha-1)} = \bigg(\frac{\alpha-1}{e}\bigg)^{\frac{\alpha-1}{2}}
$$

To find the maximum of $x^2f^*(x)$ we follow the same procedure, and find that it attains its maximum for $x=\alpha+1$. Thus,
$$
  b_+ = \sqrt{(\alpha+1)^2f^*(\alpha+1)} = \bigg(\frac{\alpha+1}{e}\bigg)^{\frac{\alpha+1}{2}}
$$
Since $f^*(x)=0$ for $x\leq0$, we immediately see that $b_- = 0$.


** b)

```{r}
# Function to generate n samples from ln(unif(0,ab))
inv.cdf.sampling = function(n, ab) {
  u = runif(n)    # Sample from unif(0,1)
  x = log(ab*u)   # Using inverse cdf
  return(x)
}

# Function to generate n samples from f
# alpha > 1
samples.f = function(n, alpha) {
  vec = rep(0,n)                        # Vector for storing the samples
  i = 1                                 # Count index in vec
  a = ((alpha-1)/exp(1))^((alpha-1)/2)  # Value of a
  b = ((alpha+1)/exp(1))^((alpha+1)/2)  # Value of b_+
  tries = 0                             # Count number of tries to generate n realizations
  while(i<=n) {
    z1 = inv.cdf.sampling(1,a)          # Sample from ln(unif(0,a))
    z2 = inv.cdf.sampling(1,b)          # Sample from ln(unif(b_-,b_+))
    diff = z2 - z1
    limit = ((alpha-1)*diff - exp(diff)) / 2  # Calculate upper limit for z1 in C_f
    if(z1 <= limit) {                   # Accept the sample
      vec[i] = exp(diff)                # Store ratio of uniforms
      i = i + 1
    }
    tries = tries + 1
  }
  return(c(tries,vec))                  # Return number of tries and vector of samples
}

# Check function
alpha = 4       # Set parameter
n = 10000       # Number of samples
gamma.sample = samples.f(n,alpha)[-1]   # Generate samples
hist(gamma.sample)                      # Histogram of samples

mean.emp = mean(gamma.sample)           # Empirical mean
mean.theo = alpha                       # Theoretical mean

var.emp = var(gamma.sample)             # Empirical variance
var.theo = alpha                        # Theoretical variance

print.mean.var(n, mean.emp, mean.theo, var.emp, var.theo) # Printing values

# Number of tries for different alphas
#alphas = seq(1.1,2000,length.out = 20)
n = 1000                                # Number of samples to generate
alphas = seq(1.1,300,length.out = 20)   # Values for alpha  
tries = rep(0,length(alphas))           # Vector to store number of tries to generate n samples for different alphas
for(i in 1:length(alphas)) {
  s = samples.f(1000,alphas[i])
  tries[i] = s[1]
}
plot(alphas,tries)                      # Plot of alpha vs number of tries
```

## 3.

```{r}
# Function to generate n samples from gamma distribution
gamma = function(n, alpha, beta) {
  if(alpha<1) {return(gamma_rejsamp(n,alpha) / beta)}       # alpha<1, beta is an inverse scale parameter
  else if(alpha>1) {return(samples.f(n,alpha)[-1] / beta)}  # alpha>1, beta is an inverse scale parameter
  else {return(samples_exp(beta,n))}                        # alpha=1, exponential distribution with rate beta
}

# Test function

# Function to generate samples and print values
test.gam = function(n, alpha, beta) {
  gam.samp = gamma(n, alpha, beta)
  mean.emp = mean(gam.samp)
  mean.theo = alpha / beta
  var.emp = var(gam.samp)
  var.theo = alpha / beta^2
  cat('alpha=',alpha,'\nbeta=',beta,'\n')
  print.mean.var(n, mean.emp, mean.theo, var.emp, var.theo)
  cat('\n\n\n')
}

# Set parameters
n = 10000
alpha = c(0.5, 1, 10)
beta = 2

test.gam(n, alpha[1], beta)
test.gam(n, alpha[2], beta)
test.gam(n, alpha[3], beta)

```


# Problem D
## 1.
```{r}
ln.accprob = function(x) {
  lnc = (15 + sqrt(53809)) / 394
  return(125*log(2+x) + 38*log(1-x) + 34*log(x) - lnc)
}

multbinom = function(n) {
  vec = rep(0,n)
  i = 1
  while(i<=n) {
    theta = runif(1)
    lnu = log(runif(1))
    lna = ln.accprob(theta)
    if(lnu<=lna) {
      vec[i] = theta
      i = i + 1
      }
  }
  return(vec)
}

# Test 
multbin.sample = multbinom(10000)
hist(multbin.sample)

mean(multbin.sample)
var(multbin.sample)
# evt regne ut teoretiske verdier?
```

