---
title: "Ex1 - TMA4300"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A

## 1.
```{r}
# Function to sample n times from exponential distribution
samples_exp = function(rate, n){
  u = runif(n)          # Sample from unif(0,1) distribution
  x = - log(u) / rate   # Compute values using inverse cdf
  return(x)             # Vector of n samples
}

# Function to print mean and variance
print.mean.var = function(n, m.emp, m.th, v.emp, v.th) {
  cat('Number of samples:', n)
  cat('\n\nEmpirical mean:', m.emp)
  cat('\nTheoretical mean:', m.th)
  cat('\n\nEmpirical variance:', v.emp)
  cat('\nTheoretical variance:', v.th)
}

# Check function
rate = 4                                        # Set rate parameter
n = 10000                                       # Number of samples
exp.sample = samples_exp(rate, n)               # Sample
hist(exp.sample)                                # Histogram of sample

mean.emp.exp = mean(exp.sample)                 # Empirical mean
mean.th.exp = 1 / rate                          # Theoretical mean

var.emp.exp = var(exp.sample)                   # Empirical variance
var.th.exp = 1 / rate^2                         # Theoretical variance

print.mean.var(n, mean.emp.exp, mean.th.exp, var.emp.exp, var.th.exp)  # Print values
```

## 2.

** a)
We consider the probability density function
$$
  g(x) = \begin{cases} 0, & x\leq 0 \\ cx^{\alpha -1}, & 0 < x < 1 \\ ce^{-x}, & x \geq 1 \end{cases},
$$
where $\alpha\in (0,1)$ and $c$ is a normalizing constant. Given $\alpha$, we find that the normalizing constant is $c=\frac{\alpha e}{\alpha + e}$.
By integrating $g(x)$ from $-\infty$ to $x$, the cumulative distribution function is found to be
$$
  F(x) = P(X\leq x) = \begin{cases} 0, & x\leq 0 \\ \frac{c}{\alpha}x^{\alpha}, & 0 < x < 1 \\ c(\frac{1}{\alpha}+\frac{1}{e}-e^{-x}), & x \geq 1 \end{cases}.
$$
In order to find the inverse cdf we let $F(x)=y$. The cdf maps $x\in(-\infty,\infty)$ to $y\in[0,1]$. By solving $F(x)=y$ with respect to $x$, we find that the inverse of the cdf is
$$
  F^{-1}(y) = \begin{cases} (\frac{\alpha y}{c})^{1/\alpha}, & 0 < y \leq \frac{c}{\alpha} \\ -\ln(\frac{1}{\alpha}+\frac{1}{e}-\frac{y}{c}), & y > \frac{c}{\alpha} \end{cases}.
$$


** b)

```{r}

samples_g = function(alpha, n) {
  c = alpha * exp(1) / (alpha + exp(1))
  u = runif(n)
  vec = rep(0, n)
  for(i in 1:length(u)){
    if(u[i]<= c/alpha) {vec[i] = (alpha * u[i] / c)^(1 / alpha)}
    else {vec[i] = -log(1/alpha + exp(-1) - u[i]/c)}
  }
  return(vec)
}

# Check solution
alpha = 0.8
c = alpha * exp(1) / (alpha + exp(1))
n = 1000
b = samples_g(alpha, n)
hist(b)

mean_emp = sum(b) / length(b)
mean_theo = c * (1/(alpha+1) + 2*exp(-1))
cat('Empirical mean:', mean_emp)
cat('\nTheoretical mean:', mean_theo)

var_emp = sum((b-mean_emp)^2) / (length(b)-1) 
var_theo = c * (1/(alpha+2) + 5*exp(-1)) - mean_theo^2
cat('\nEmpirical variance:', var_emp)
cat('\nTheoretical variance:', var_theo)
```

## 3.

```{r}

boxmuller = function(n) {
  u1 = runif(n)
  u2 = runif(n)
  r = sqrt(-2*log(u1))
  theta = 2*pi*u2
  x = r*cos(theta)
  #y = r*sin(theta)
  return(x)
}
bm = boxmuller(10000)
hist(bm)

mean_bm = sum(bm) / length(bm)
cat('Mean:', mean_bm)

var_bm = sum((bm-mean_bm)^2) / (length(bm)-1)
cat('\nVariance:', var_bm)
```


## 4.

```{r}
mult_normal = function(d, mu, sigma) {
  # mu must have length d, sigma dxd
  x = boxmuller(d)
  A = t(chol(sigma))  # lower triangular matrix
  y = mu + A %*% x    # y mult normal
  return(y)
}

# Check solution
mu = c(1,2)
sigma = cbind(c(1,0.5),c(0.5,1))

# Estimate mean and empirical variance
n = 1000
mu_est = rep(0,length(mu))
multnor.sample = matrix(NA,n,length(mu))
for(i in 1:n) {
  multnor = mult_normal(length(mu),mu,sigma)
  mu_est = mu_est + multnor
  multnor.sample[i,] = multnor
  }
mu_est = mu_est / n
mu_est

var(multnor.sample)

```


# Problem B
## 1.
** a)

The acceptance probability is
$$
  a = \begin{cases} e^{-x}, & 0<x<1 \\ x^{\alpha-1}, & x\geq 1 \end{cases}.
$$

** b)

```{r}
accprob = function(x,alpha) {
  if(x<1) {return(exp(-x))}
  else if(x>=1) {return(x^(alpha-1))}
}

gamma_rejsamp = function(n,alpha) {
  vec = rep(0,n)
  i = 1
  while(i<=n) {
    x = samples_g(alpha,1)
    u = runif(1)
    a = accprob(x,alpha)
    if(u<=a) {
      vec[i] = x
      i = i + 1
      }
  }
  return(vec)
}

alpha = 0.5
n = 10000
gamma.sample = gamma_rejsamp(n,alpha)
mean.emp = mean(gamma.sample)
mean.emp
mean.theo = alpha
mean.theo
var.emp = var(gamma.sample)
var.emp
var.theo = alpha
var.theo

```


## 2.

** a)

$$
  a = \bigg(\frac{\alpha-1}{e}\bigg)^{\frac{\alpha-1}{2}}
$$
$b_- = 0$ and

$$
  b_+ = \bigg(\frac{\alpha+1}{e}\bigg)^{\frac{\alpha+1}{2}}
$$

** b)

```{r}
inv.cdf.sampling = function(n,ab) {
  u = runif(n)
  x = log(ab*u)
  return(x)
}

samples.f = function(n,alpha) {
  vec = rep(0,n)
  i = 1
  a = ((alpha-1)/exp(1))^((alpha-1)/2)
  b = ((alpha+1)/exp(1))^((alpha+1)/2)
  tries = 0
  while(i<=n) {
    z1 = inv.cdf.sampling(1,a)
    z2 = inv.cdf.sampling(1,b)
    diff = z2 - z1
    limit = ((alpha-1)*diff - exp(diff)) / 2
    limit
    if(z1 <= limit) {
      vec[i] = exp(diff)
      i = i + 1
    }
    tries = tries + 1
  }
  return(c(tries,vec))
}

#Check function
alpha = 4
n = 10000
gamma.sample = samples.f(n,alpha)[-1]
mean.emp = mean(gamma.sample)
mean.emp
mean.theo = alpha
mean.theo
var.emp = var(gamma.sample)
var.emp
var.theo = alpha
var.theo


# Number of tries for different alphas
#alphas = seq(1,2000,length.out = 20)
alphas = seq(1,300,length.out = 20)
tries = rep(0,length(alphas))
for(i in 1:length(alphas)) {
  s = samples.f(1000,alphas[i])
  tries[i] = s[1]
}
plot(alphas,tries)
```

## 3.

```{r}

gamma = function(n,alpha,beta) {
  if(alpha<1) {return(gamma_rejsamp(n,alpha) / beta)}
  else if(alpha>1) {return(samples.f(n,alpha)[-1] / beta)}
  else {return(samples_exp(beta,n))}
}

# Test function
alpha = 0.5
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

alpha = 1
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

alpha = 10
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

```
# Problem C

We define
$$x_k = \frac{z_k}{z_1 + \ldots + z_K},$$ where $z_k$ is gamma distributed with parameters $\alpha_k, 1$. Assuming the $z_k;\;k=1,\ldots,K$ are independent, they have joint distribution:
$$f_Z(z_1,\ldots,z_K;\alpha_1,\ldots,\alpha_K,1) = \prod_{k=1}^{K}\bigg(\frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}e^{-z_k}\bigg).$$
We start by defining a mapping $g:(z_1,\ldots,z_K)\rightarrow(x_1,\ldots,x_{K-1},v),$ where $v = z_1+\ldots+z_K$:
$$(x_1,\ldots,x_{K-1},v) = g(z_1,\ldots,z_K) = \big(\frac{z_1}{\sum_{k=1}^Kz_k},\ldots,\frac{z_{K-1}}{\sum_{k=1}^Kz_k},\sum_{k=1}^Kz_k\big).$$
Then $(z_1,\ldots, z_K) = g^{-1}(x_1,\ldots,x_{K-1},v) = \big(x_1\cdot v,\ldots,x_{K-1}\cdot v, (1 - \sum_{k=1}^{K-1}x_k)\cdot v\big).$
From the change of variables theorem, we have that, provided a relation $(x_,\ldots,x_n) = g(z_1,\ldots,z_n)$,
$$f_X(x_1,\ldots,x_n) = f_Z\big(g^{-1}(x_1,\ldots,x_n)\big)\mid\mathbf{J}\mid,$$
where $\mid\mathbf{J}\mid$ is the Jacobian of $g^{-1}$ with respect to $\mathbf{X}.$ In our case, this is:
$$\mathbf{J} = \begin{bmatrix} \frac{\partial g^{-1}_1}{\partial x_1}& \cdots & \frac{\partial g_1^{-1}}{\partial x_{K-1}} & \frac{\partial g_1^{-1}}{\partial v}\\
\vdots & \ddots & & \vdots\\
\frac{\partial g^{-1}_K}{\partial x_1} & \cdots & \frac{\partial g^{-1}_K}{\partial x_{K-1}} & \frac{\partial g^{-1}_K}{\partial v}
\end{bmatrix},$$
Looking at the different elements of the Jacobian:
$$\frac{\partial g_i^{-1}}{\partial x_j} = \frac{\partial}{\partial x_j}x_i\cdot v = \{v \::\: i=j,0\::\:\text{otherwise}\},$$
$$\frac{\partial g_K^{-1}}{\partial x_i} = \frac{\partial}{\partial x_i}(1 - \sum_{k=1}^{K-1}x_k)\cdot v = -v,$$
$$\frac{\partial g_K^{-1}}{\partial v} = \frac{\partial}{\partial v}(1 - \sum_{k=1}^{K-1}x_k)\cdot v = (1 - \sum_{k=1}^{K-1}x_k),$$

$$\frac{\partial g_k^{-1}}{\partial v} = \frac{\partial}{\partial v} x_k\cdot v = x_k.$$ This gives us the Jacobi matrix 

$$\mathbf{J} = \begin{bmatrix} v& 0 & \cdots & 0 & x_1\\
\vdots & \ddots & & & \vdots\\
0 & \cdots & \cdots & v & x_{K-1}\\
-v & \cdots & \cdots & -v & (1 - \sum_{k=1}^{K-1}x_k)
\end{bmatrix},$$
and we need the determinant of this. We observe that this matrix is a block matrix on the form 
$$\begin{bmatrix}
A & B \\
C & D
\end{bmatrix},$$
where A is a $K-1\times K-1$ diagonal matrix with $v$ on the diagonals, B is $[x_1,\ldots,x_{K-1}]^\text{T},$ C is the $1\times K- 1$ vector $[-v,\ldots,-v]$ and D is $(1 - \sum_{k=1}^{K-1}x_k)$. For this type of matrices, we have that 
$$\det\bigg(\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}\bigg) = (D - CA^{-1}B)\det(A)$$. 
Since A is diagonal with entries $v$, $\det(A) = v^{K-1}.$ Furthermore, A^{-1} is a diagonal matric with $1/v$ as entries. This gives us
$$D - CA^{-1}B = (1 - \sum_{k=1}^{K-1}x_k) - [-v,\ldots,-v]\times\begin{bmatrix}\frac{1}{v}&&0\\&\ddots & \\ 0& & \frac{1}{v}\end{bmatrix} \times[x_1,\ldots,x_{K-1}]^{\text{T}}$$
$$=1 - \sum_{k=1}^{K-1}x_k -(-\sum_{k=1}^{K-1}x_k) = 1.$$
We get 
$$\mid\mathbf{J}\mid = v^{K-1}.$$
From the change of variable theorem, we now get
$$
\begin{equation}
  f_X(x_1,\ldots,x_{K-1},v) = f_Z(x_1\cdot v, \ldots, x_{K-1}\cdot v,(1 - \sum_{k=1}^{K-1}x_k)\cdot v)\cdot v^{K-1}\\
  = e^{-(1 - \sum_{k=1}^{K-1}x_k)v}\frac{((1 - \sum_{k=1}^{K-1}x_k)v)^{\alpha_K-1}}{\Gamma(\alpha_K)}\prod_{k=1}^{K-1}\bigg(\frac{(v\cdot x_k)^{\alpha_k-1}}{\Gamma(\alpha_k)}e^{-x_k\cdot v}\bigg)\cdot v^{K-1}\\
  =e^{-v(1 - \sum_{k=1}^{K-1}x_k + \sum_{k=1}^{K-1}x_k)}\cdot v^{K-1 + \sum_{k=1}^{K}(\alpha_k -1)}\cdot \frac{(1 - \sum_{k=1}^{K-1}x_k)^{\alpha_K-1}}{\Gamma(\alpha_K)}\prod_{k=1}^{K-1}\frac{x_k^{\alpha_k -1}}{\Gamma(\alpha_k)}\\
  =e^{-v}\cdot v^{\sum_{k=1}^{K}(\alpha_k) - 1}\cdot \frac{(1 - \sum_{k=1}^{K-1}x_k)^{\alpha_K-1}}{\Gamma(\alpha_K)}\prod_{k=1}^{K-1}\frac{x_k^{\alpha_k -1}}{\Gamma(\alpha_k)}
  
\end{equation}
$$
We now have the distribution of $(x_1,\ldots,x_{K-1},v)$ and integrate this over $v$ to get the desired distribution of $(x_1,\ldots,x_{K-1})$. As only the first two factors are dependent on $v$, we look at
$$\int_0^\infty e^{-v}\cdot v^{\sum_{k=1}^{K}(\alpha_k) - 1}\text{d}v = \Gamma(\sum_{k=1}^{K}\alpha_k).$$
Inserted into the distribution, we get
$$\begin{equation}
f_X(x_1,\ldots,x_{K-1}) = \Gamma(\sum_{k=1}^{K}\alpha_k)\cdot \frac{(1 - \sum_{k=1}^{K-1}x_k)^{\alpha_K-1}}{\Gamma(\alpha_K)}\prod_{k=1}^{K-1}\frac{x_k^{\alpha_k -1}}{\Gamma(\alpha_k)},
\end{equation}
$$
which is the Dirichlet distribution with parameters $(\alpha_1,\ldots,\alpha_K).$

## 2.

```{r}
#Function for generating one Diriclet distributed variable with parameter alpha:
dirichlet <- function(alphas){
  zs = rep(0,length(alphas))
  for (k in 1:length(alphas)){
    zs[k] <- gamma(1,alphas[k],1)
  }
  return(zs/sum(zs))
}

#Testing
K = 3
alphas <- runif(K,0.1,5.0)
mean.theo <- alphas/sum(alphas)
dirichlet.samples <- rep(0,K)
for (i in 1:30){dirichlet.samples = dirichlet.samples + dirichlet(alphas)}
dirichlet.samples = dirichlet.samples/30
mean.theo
dirichlet.samples

```


# Problem D
## 1.
```{r}
ln.accprob = function(x) {
  lnc = (15 + sqrt(53809)) / 394
  return(125*log(2+x) + 38*log(1-x) + 34*log(x) - lnc)
}

multbinom = function(n) {
  vec = rep(0,n)
  i = 1
  while(i<=n) {
    theta = runif(1)
    lnu = log(runif(1))
    lna = ln.accprob(theta)
    if(lnu<=lna) {
      vec[i] = theta
      i = i + 1
      }
  }
  return(vec)
}

# Test 
multbin.sample = multbinom(10000)
multbin.mean.MC <- mean(multbin.sample)

hist(multbin.sample)
abline(v = multbin.mean.MC, col="red")

mean(multbin.sample)
var(multbin.sample)
# evt regne ut teoretiske verdier? 
```

##2

```{r}
multbin.prior <- function(x){
  return((2+x)^125*(1-x)^38*x^34)
}

multbin.integrand <- function(x){
  c <- integrate(multbin.prior,0,1)
  return((1/(c$value))*x*(2+x)^125*(1-x)^38*x^34)
}

multbin.mean.exact <- integrate(multbin.integrand,0,1)
multbin.mean.exact
```