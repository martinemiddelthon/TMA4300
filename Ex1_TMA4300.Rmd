---
title: "Ex1 - TMA4300"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A

## 1.
```{r}
# Function to sample n times from exponential distribution
samples_exp = function(rate, n){
  u = runif(n)          # Sample from unif(0,1) distribution
  x = - log(u) / rate   # Compute values using inverse cdf
  return(x)             # Vector of n samples
}

# Function to print mean and variance
print.mean.var = function(n, m.emp, m.th, v.emp, v.th) {
  cat('Number of samples:', n)
  cat('\n\nEmpirical mean:', m.emp)
  cat('\nTheoretical mean:', m.th)
  cat('\n\nEmpirical variance:', v.emp)
  cat('\nTheoretical variance:', v.th)
}

# Check function
rate = 4                                        # Set rate parameter
n = 10000                                       # Number of samples
exp.sample = samples_exp(rate, n)               # Sample
hist(exp.sample)                                # Histogram of sample

mean.emp.exp = mean(exp.sample)                 # Empirical mean
mean.th.exp = 1 / rate                          # Theoretical mean

var.emp.exp = var(exp.sample)                   # Empirical variance
var.th.exp = 1 / rate^2                         # Theoretical variance

print.mean.var(n, mean.emp.exp, mean.th.exp, var.emp.exp, var.th.exp)  # Print values
```

## 2.

** a)
We consider the probability density function
$$
  g(x) = \begin{cases} 0, & x\leq 0 \\ cx^{\alpha -1}, & 0 < x < 1 \\ ce^{-x}, & x \geq 1 \end{cases},
$$
where $\alpha\in (0,1)$ and $c$ is a normalizing constant. Given $\alpha$, we find that the normalizing constant is $c=\frac{\alpha e}{\alpha + e}$.
By integrating $g(x)$ from $-\infty$ to $x$, the cumulative distribution function is found to be
$$
  F(x) = P(X\leq x) = \begin{cases} 0, & x\leq 0 \\ \frac{c}{\alpha}x^{\alpha}, & 0 < x < 1 \\ c(\frac{1}{\alpha}+\frac{1}{e}-e^{-x}), & x \geq 1 \end{cases}.
$$
In order to find the inverse cdf we let $F(x)=y$. The cdf maps $x\in(-\infty,\infty)$ to $y\in[0,1]$. By solving $F(x)=y$ with respect to $x$, we find that the inverse of the cdf is
$$
  F^{-1}(y) = \begin{cases} (\frac{\alpha y}{c})^{1/\alpha}, & 0 < y \leq \frac{c}{\alpha} \\ -\ln(\frac{1}{\alpha}+\frac{1}{e}-\frac{y}{c}), & y > \frac{c}{\alpha} \end{cases}.
$$


** b)

```{r}

samples_g = function(alpha, n) {
  c = alpha * exp(1) / (alpha + exp(1))
  u = runif(n)
  vec = rep(0, n)
  for(i in 1:length(u)){
    if(u[i]<= c/alpha) {vec[i] = (alpha * u[i] / c)^(1 / alpha)}
    else {vec[i] = -log(1/alpha + exp(-1) - u[i]/c)}
  }
  return(vec)
}

# Check solution
alpha = 0.8
c = alpha * exp(1) / (alpha + exp(1))
n = 1000
b = samples_g(alpha, n)
hist(b)

mean_emp = sum(b) / length(b)
mean_theo = c * (1/(alpha+1) + 2*exp(-1))
cat('Empirical mean:', mean_emp)
cat('\nTheoretical mean:', mean_theo)

var_emp = sum((b-mean_emp)^2) / (length(b)-1) 
var_theo = c * (1/(alpha+2) + 5*exp(-1)) - mean_theo^2
cat('\nEmpirical variance:', var_emp)
cat('\nTheoretical variance:', var_theo)
```

## 3.

```{r}

boxmuller = function(n) {
  u1 = runif(n)
  u2 = runif(n)
  r = sqrt(-2*log(u1))
  theta = 2*pi*u2
  x = r*cos(theta)
  #y = r*sin(theta)
  return(x)
}
bm = boxmuller(10000)
hist(bm)

mean_bm = sum(bm) / length(bm)
cat('Mean:', mean_bm)

var_bm = sum((bm-mean_bm)^2) / (length(bm)-1)
cat('\nVariance:', var_bm)
```


## 4.

```{r}
mult_normal = function(d, mu, sigma) {
  # mu must have length d, sigma dxd
  x = boxmuller(d)
  A = t(chol(sigma))  # lower triangular matrix
  y = mu + A %*% x    # y mult normal
  return(y)
}

# Check solution
mu = c(1,2)
sigma = cbind(c(1,0.5),c(0.5,1))

# Estimate mean and empirical variance
n = 1000
mu_est = rep(0,length(mu))
multnor.sample = matrix(NA,n,length(mu))
for(i in 1:n) {
  multnor = mult_normal(length(mu),mu,sigma)
  mu_est = mu_est + multnor
  multnor.sample[i,] = multnor
  }
mu_est = mu_est / n
mu_est

var(multnor.sample)

```


# Problem B
## 1.
** a)

The acceptance probability is
$$
  a = \begin{cases} e^{-x}, & 0<x<1 \\ x^{\alpha-1}, & x\geq 1 \end{cases}.
$$

** b)

```{r}
accprob = function(x,alpha) {
  if(x<1) {return(exp(-x))}
  else if(x>=1) {return(x^(alpha-1))}
}

gamma_rejsamp = function(n,alpha) {
  vec = rep(0,n)
  i = 1
  while(i<=n) {
    x = samples_g(alpha,1)
    u = runif(1)
    a = accprob(x,alpha)
    if(u<=a) {
      vec[i] = x
      i = i + 1
      }
  }
  return(vec)
}

alpha = 0.5
n = 10000
gamma.sample = gamma_rejsamp(n,alpha)
mean.emp = mean(gamma.sample)
mean.emp
mean.theo = alpha
mean.theo
var.emp = var(gamma.sample)
var.emp
var.theo = alpha
var.theo

```


## 2.

** a)

$$
  a = \bigg(\frac{\alpha-1}{e}\bigg)^{\frac{\alpha-1}{2}}
$$
$b_- = 0$ and

$$
  b_+ = \bigg(\frac{\alpha+1}{e}\bigg)^{\frac{\alpha+1}{2}}
$$

** b)

```{r}
inv.cdf.sampling = function(n,ab) {
  u = runif(n)
  x = log(ab*u)
  return(x)
}

samples.f = function(n,alpha) {
  vec = rep(0,n)
  i = 1
  a = ((alpha-1)/exp(1))^((alpha-1)/2)
  b = ((alpha+1)/exp(1))^((alpha+1)/2)
  tries = 0
  while(i<=n) {
    z1 = inv.cdf.sampling(1,a)
    z2 = inv.cdf.sampling(1,b)
    diff = z2 - z1
    limit = ((alpha-1)*diff - exp(diff)) / 2
    limit
    if(z1 <= limit) {
      vec[i] = exp(diff)
      i = i + 1
    }
    tries = tries + 1
  }
  return(c(tries,vec))
}

#Check function
alpha = 4
n = 10000
gamma.sample = samples.f(n,alpha)[-1]
mean.emp = mean(gamma.sample)
mean.emp
mean.theo = alpha
mean.theo
var.emp = var(gamma.sample)
var.emp
var.theo = alpha
var.theo


# Number of tries for different alphas
#alphas = seq(1,2000,length.out = 20)
alphas = seq(1,300,length.out = 20)
tries = rep(0,length(alphas))
for(i in 1:length(alphas)) {
  s = samples.f(1000,alphas[i])
  tries[i] = s[1]
}
plot(alphas,tries)
```

## 3.

```{r}

gamma = function(n,alpha,beta) {
  if(alpha<1) {return(gamma_rejsamp(n,alpha) / beta)}
  else if(alpha>1) {return(samples.f(n,alpha)[-1] / beta)}
  else {return(samples_exp(beta,n))}
}

# Test function
alpha = 0.5
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

alpha = 1
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

alpha = 10
beta = 2
n = 1000
gam.samp = gamma(n,alpha,beta)
mean.emp = mean(gam.samp)
mean.theo = alpha / beta
mean.emp
mean.theo

var.emp = var(gam.samp)
var.theo = alpha / beta^2
var.emp
var.theo

```


# Problem D
## 1.
```{r}
ln.accprob = function(x) {
  lnc = (15 + sqrt(53809)) / 394
  return(125*log(2+x) + 38*log(1-x) + 34*log(x) - lnc)
}

multbinom = function(n) {
  vec = rep(0,n)
  i = 1
  while(i<=n) {
    theta = runif(1)
    lnu = log(runif(1))
    lna = ln.accprob(theta)
    if(lnu<=lna) {
      vec[i] = theta
      i = i + 1
      }
  }
  return(vec)
}

# Test 
multbin.sample = multbinom(10000)
hist(multbin.sample)

mean(multbin.sample)
var(multbin.sample)
# evt regne ut teoretiske verdier?
```

