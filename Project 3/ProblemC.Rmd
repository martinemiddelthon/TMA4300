---
title: "ProblemC"
author: "Martine Middelthon"
date: "8 4 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem C

We have $x_i$ and $y_i$, $i=1,\ldots,n$, which are independent random variables. Furthermore, the $x_i$'s and $y_i$'s have an exponential distribution with parameter $\lambda_0$ and $\lambda_1$, respectively. Instead of observing these variables directly, we observe $z_i=\max(x_i,y_i)$ and $u_i=I(x_i\geq y_i)$ for $i=1,\ldots,n$.

## 1.

The joint distribution of the variables is

$$
  f(\mathbf{x}, \mathbf{y} \mid \lambda_0, \lambda_1) = \prod_{i=1}^n f(x_i\mid\lambda_0)\prod_{i=1}^n f(y_i\mid\lambda_1) = (\lambda_0\lambda_1)^n \prod_{i=1}^n e^{-\lambda_0x_i-\lambda_1y_i} \quad.
$$

We take the logarithm of this and get

$$
  \ln f(\mathbf{x}, \mathbf{y} \mid \lambda_0, \lambda_1) = n(\ln\lambda_0+\ln\lambda_1) -\lambda_0\sum_{i=1}^n x_i -\lambda_1\sum_{i=1}^n y_i \quad . 
$$

We are interested in the expectation $E(\ln f(\mathbf{x}, \mathbf{y} \mid \lambda_0, \lambda_1)\mid \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)}) $, so we need to find $E(x_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)})$ and $E(y_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)})$.
We begin by finding the distribution $f(x_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)})$, and observe that we can divide it into two cases based on the observed $u_i$. We have

$$
\begin{aligned}
  f(x_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) &= \begin{cases} z_i, & u_i=1 \\ \frac{f(z_i\mid x_i)f(x_i)}{f(z_i)}, & u_i=0 \end{cases} \\
  &= \begin{cases} z_i, & u_i=1 \\ \frac{\lambda_0^{(t)}\exp(-\lambda_0^{(t)}x_i)}{1-\exp(-\lambda_0^{(t)}z_i)}, & u_i=0 \end{cases} \quad.
\end{aligned}
$$

Since $E(x)=\int xf(x)dx $, we get

$$
\begin{aligned}
  E(x_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) &= u_i z_i+ (1-u_i)\int_0^{z_i} x_i \frac{\lambda_0^{(t)}\exp(-\lambda_0^{(t)}x_i)}{1-\exp(-\lambda_0^{(t)}z_i)} dx_i \\
  &= u_iz_i + (1-u_i) \bigg(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1} \bigg)
\end{aligned}\quad,
$$

where we have used the method of integration by parts.
We follow the same procedure in order to find $E(y_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)})$. We find the distribution

$$
  \begin{aligned}
  f(y_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) &= \begin{cases} z_i, & u_i=0 \\ \frac{f(z_i\mid y_i)f(y_i)}{f(z_i)}, & u_i=1 \end{cases} \\
  &= \begin{cases} z_i, & u_i=0 \\ \frac{\lambda_1^{(t)}\exp(-\lambda_1^{(t)}y_i)}{1-\exp(-\lambda_1^{(t)}z_i)}, & u_i=1 \end{cases} \quad.
\end{aligned} \quad.
$$

Then we find the expectation

$$
\begin{aligned}
  E(y_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) &= (1-u_i) z_i+ u_i\int_0^{z_i} y_i \frac{\lambda_1^{(t)}\exp(-\lambda_1^{(t)}y_i)}{1-\exp(-\lambda_1^{(t)}z_i)} dy_i \\
  &= (1-u_i)z_i + u_i \bigg(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1} \bigg)
\end{aligned}\quad.
$$

Thus we get

$$
\begin{aligned}
  E(\ln f(\mathbf{x}, \mathbf{y} \mid \lambda_0, \lambda_1)\mid \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)})
  &= E\bigg(n(\ln\lambda_0+\ln\lambda_1) -\lambda_0\sum_{i=1}^n x_i -\lambda_1\sum_{i=1}^n y_i\bigg) \\
  &= n(\ln\lambda_0+\ln\lambda_1) -\lambda_0\sum_{i=1}^n E(x_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) -\lambda_1\sum_{i=1}^n E(y_i\mid z_i,u_i,\lambda_0^{(t)},\lambda_1^{(t)}) \\
  &= n(\ln\lambda_0+\ln\lambda_1) -\lambda_0\sum_{i=1}^n \bigg\{u_iz_i + (1-u_i) \bigg(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1} \bigg)\bigg\} -\lambda_1\sum_{i=1}^n \bigg\{(1-u_i)z_i + u_i \bigg(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\bigg)\bigg\}
\end{aligned}\quad.
  
$$


## 2.

We want to use the EM algorithm to find the maximum likelihood estimates of $\lambda_0$ and $\lambda_1$. We denote $Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)},\lambda_1^{(t)}) = E(\ln f(\mathbf{x}, \mathbf{y} \mid \lambda_0, \lambda_1)\mid \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)})$ and want to maximize $Q$ with respect to $\lambda_0$ and $\lambda_1$. We calculate

$$
  \frac{\partial Q}{\partial\lambda_0} = \frac{n}{\lambda_0} - \sum_{i=1}^n \bigg\{u_iz_i + (1-u_i) \bigg(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1} \bigg)\bigg\} = 0 \\
  \implies \lambda_0 = \frac{n}{\sum_{i=1}^n \bigg\{u_iz_i + (1-u_i) \bigg(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1} \bigg)\bigg\}}
$$
and

$$
  \frac{\partial Q}{\partial\lambda_1} = \frac{n}{\lambda_1} - \sum_{i=1}^n \bigg\{(1-u_i)z_i + u_i \bigg(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\bigg)\bigg\} =0 \\
  \implies \lambda_1 = \frac{n}{\sum_{i=1}^n \bigg\{(1-u_i)z_i + u_i \bigg(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\bigg)\bigg\}}.
$$

Hence we use the recursions

$$
  \lambda_0^{(t+1)} = \frac{n}{\sum_{i=1}^n \bigg\{u_iz_i + (1-u_i) \bigg(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp(\lambda_0^{(t)}z_i)-1} \bigg)\bigg\}} \\
  \lambda_1^{(t+1)} = \frac{n}{\sum_{i=1}^n \bigg\{(1-u_i)z_i + u_i \bigg(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp(\lambda_1^{(t)}z_i)-1}\bigg)\bigg\}}.
$$

In the algorithm below we terminate the recursion when $|Q^{(t+1)}-Q^{(t)}|\leq \epsilon$. Setting $\epsilon=10^{-8}$ and initial values $(\lambda_0,\lambda_1)_0=(2,2)$, we get the maximum likelihood estimates $(\lambda_0,\lambda_1)_{MLE}=(3.46,9.33)$.

```{r}
# Load data
u = read.delim("u.txt")
z = read.delim("z.txt")
u = u[[1]]
z = z[[1]]

# Function to calculate the sums in the expressions above
summs = function(u, z, lambda0t, lambda1t) {
  n = length(u)
  summ0 = 0
  summ1 = 0
  for (i in (1:n)) {
    summ0 = summ0 + u[i] * z[i] + (1-u[i]) * (1/lambda0t - z[i]/(exp(lambda0t*z[i])-1))
    summ1 = summ1 + (1-u[i]) * z[i] + u[i] * (1/lambda1t- z[i]/(exp(lambda1t*z[i])-1))
  }
  return(c(summ0,summ1))
}

# Function fo calculate Q
Q = function(u, z, lambda0t, lambda1t) {
  #n = length(y)
  n = length(u)   # DEBUG 
  zums = summs(u,z,lambda0t, lambda1t)
  expect = n * (log(lambda0t)+log(lambda1t)) - lambda0t * zums[1] - lambda1t * zums[2]
  return(expect)
}

# EM algorithm for the problem
# returns the maximum likelihood estimators for a given threshold epsilon
# produces convergence plots
EM = function(u, z, lambda.init, epsilon, plot=TRUE) {
  n = length(u)
  lambda0 = c(lambda.init[1]) # Create vector and set the first value
  lambda1 = c(lambda.init[2]) # Create vector and set the first value
  # To initiate the while loop
  Q.t = -Inf
  Q.t1 = Q(u, z, lambda0[1], lambda1[1])
  t = 1
  while (abs(Q.t1 - Q.t) > epsilon) {           # Terminate if abs(Q^{t+1}-Q^{t} <= epsilon)
    zums = summs(u, z, lambda0[t], lambda1[t])
    lambda0 = c(lambda0, n/zums[1])
    lambda1 = c(lambda1, n/zums[2])
    Q.t = Q.t1
    Q.t1 = Q(u, z, lambda0[t+1], lambda1[t+1])
    t = t + 1
  }
  if(plot){
    plot((1:length(lambda0)), lambda0, xlab = "t", ylab = bquote(lambda[0]), main = bquote("Convergence for"~lambda[0]))  # Convergence plot
    plot((1:length(lambda1)), lambda1, xlab = "t", ylab = bquote(lambda[1]), main = bquote("Convergence for"~lambda[1]))  # COnvergence plot
  }
  
  return(c(lambda0[length(lambda0)], lambda1[length(lambda1)]))
}

#EMfor = function(u, z, lambda.init, it) {
#  n = length(u)
#  lambda0 = c(lambda.init[1])
#  lambda1 = c(lambda.init[2])
#  for (t in (1:it)) {
#     zums = summs(u, z, lambda0[t], lambda1[t])
#    lambda0 = c(lambda0, n/zums[1])
#    lambda1 = c(lambda1, n/zums[2])
#  }
  
#  plot((1:length(lambda0)), lambda0)
#  plot((1:length(lambda1)), lambda1)
  
#  return(c(lambda0[length(lambda0)], lambda1[length(lambda1)]))
#}


result = EM(u, z, c(2, 2), 0.00000001)
result
```
The bootstrap method is used to find $SD(\hat{\lambda}_0), SD(\hat{\lambda}_1)$ and $Corr[\hat{\lambda}_0,\hat{\lambda}_1].$ We get the results 
$$
SD(\hat{\lambda}_0) = 0.26,\; SD(\hat{\lambda}_1) = 0.81,\;Corr[\hat{\lambda}_0,\hat{\lambda}_1] = 0.06. 
$$

```{r}
u = read.delim("u.txt")
z = read.delim("z.txt")
u = u[[1]]
z = z[[1]]

# Create B bootstrap samples: 
B = 1000
u.boot <- matrix(0L, nrow=B, ncol=length(u))
z.boot <- matrix(0L, nrow=B, ncol=length(u))

# for(b in 1:B){
#   u.boot[b,] <- sample(u, length(u), replace=TRUE)
#   z.boot[b,] <- sample(z, length(z), replace=TRUE)
# }

for(b in 1:B){
  idx <- sample(seq(1:length(u)), length(u), replace=TRUE)
  u.boot[b,] <- u[idx]
  z.boot[b,] <- z[idx]
}

# Evaluate corresponding parameter estimates:
lambda0.boot <- rep(0,B)
lambda1.boot <- rep(0,B)

for(b in 1:B){
  lambs <- EM(u.boot[b,], z.boot[b,], c(2,2),0.00000001, plot=FALSE)
  lambda0.boot[b] <- lambs[1]
  lambda1.boot[b] <- lambs[2]
}

# estimate sd and corr:
lambda0.sd <- sd(lambda0.boot); lambda0.sd
lambda1.sd <- sd(lambda1.boot); lambda1.sd
lambda.corr <- cor(lambda0.boot, lambda1.boot); lambda.corr
cat(c(lambda0.sd, lambda1.sd, lambda.corr))
```
The pseudocode for the bootstrap method for obtaining $SD(\hat{\lambda}_0), SD(\hat{\lambda}_1)$ and $Corr[\hat{\lambda}_0,\hat{\lambda}_1]$ is shown below. 
$$
1. \quad \text{Generate B bootstrap samples of pairs of u and z;}
$$
$$
(u,z)^{*(1)},\ldots,(u,z)^{*(B)},
$$
$$
\text{by sampling from $u$ and $z$ with replacement.}
$$
$$
2. \quad \text{Evaluate the corresponding parameter estimates}\\
\text{by using the EM-function with } (u,z)^{*(b)} \text{ as input parameters:}
$$
$$
\hat{\lambda_0}^{*(1)},\ldots,\hat{\lambda_0}^{*(B)},\hat{\lambda_1}^{*(1)},\ldots, \hat{\lambda_1}^{*(B)}.
$$
$$
3. \quad \text{Estimate the standard deviation of }\hat{\lambda_0} \text{ and } \hat{\lambda_1} \text{ by:}
$$
$$
\hat{SE}_B = \sqrt{\frac{\sum_{b=1}^B(\hat{\lambda}^{*(b)} - \hat{\lambda}^{*(\cdot)})^2}{B-1}},
$$
$$
\text{where } \hat{\lambda}^{*(\cdot)} = \frac{1}{B}\sum_{b=1}^B\hat{\lambda}^{*(b)}.
$$
$$
4. \quad \text{ Estimate }Corr[\hat{\lambda_0}, \hat{\lambda_1}] \text{ by }
$$

$$
Corr[\hat{\lambda_0}, \hat{\lambda_1}] = \frac{Cov(\hat{\lambda_0}, \hat{\lambda_1})}{Var(\hat{\lambda_0})Var(\hat{\lambda_1})},\\
\text{where}
$$
$$
Cov(\hat{\lambda_0},\hat{\lambda_1}) = \frac{\sum_{b=1}^B(\hat{\lambda_0}^{*(b)} - \hat{\lambda_0}^{*(\cdot)})(\hat{\lambda_1}^{*(b)} - \hat{\lambda_1}^{*(\cdot)})}{N-1}\\
\text{and}
$$
$$
Var(\hat{\lambda_0}) = SD(\hat{\lambda_0})^2;\;Var(\hat{\lambda_1}) = SD(\hat{\lambda_1})^2.
$$

TODO: Implementer bias og diskuter om du foretrekker bias-corrected eller ikke. 

## 4.
We will now try to estimate $\lambda_0$ and $\lambda_1$ by the maximum likelihood estimator instead. To find the likelihood of $\lambda_0$ and $\lambda_1$, and from this, their maximum likelihood estimates, we first need to find
$$
f_{\mathbf{U}, \mathbf{Z}}(\mathbf{u},\mathbf{z}\mid \lambda_0,\lambda_1).
$$
Since the pairs $(u_i,z_i)$ are i.i.d, we have that 
$$
f_{\mathbf{U}, \mathbf{Z}}(\mathbf{u},\mathbf{z}\mid \lambda_0,\lambda_1) = \prod_{i=1}^n f_{U_i,Z_i}(u_i,z_i \mid \lambda_0, \lambda_1),
$$
so we find the expression for $f_{U_i,Z_i}(u_i,z_i \mid \lambda_0, \lambda_1)$. To do this, we consider the cases where $u_i=0$ and $u_i=1$ separately, starting with $u_i=0$. We recall that for $u_1=0,$ we have that $z_i = y_i$ and $y_i \geq x_i,$ which gives us 
$$
F_{Z_i}(z_i,u_1=0\mid\lambda_0,\lambda_1) = \mathbb{P}(Z_i \leq z_i, U_i=0) = \mathbb{P}(Y_i\leq z_i, X_i\leq y_i) \\
= \int_0^{z_i}\int_0^{y_i}f_{X_i,Y_i}(x_i,y_1\mid \lambda_0,\lambda_1) \text{d}x_i\text{d}y_i \\
= \int_0^{z_i}\int_0^{y_i}f_{X_i}(x_i\mid \lambda_0,\lambda_1)f_{Y_i}(y_i\mid \lambda_0,\lambda_1) \text{d}x_i\text{d}y_i \\
=\int_0^{z_i}\int_0^{y_i} \lambda_0e^{-\lambda_0x_i}\lambda_1e^{\lambda_1y_i} \text{d}x_i\text{d}y_i \\
= \int_0^{z_i}\lambda_1e^{\lambda_1y_1} - \lambda_1e^{-y_i(\lambda_0 + \lambda_1)} \text{d}y_i\\
= \frac{\lambda_1}{\lambda_0 + \lambda_1}e^{-z_i(\lambda_0 + \lambda_1)} - e^{\lambda_1z_i} + C.
$$
Differentiating this, we get the density 
$$
f_{Z_i}(z_i, u_i = 0 \mid \lambda_0, \lambda_1) = \frac{\text{d}}{\text{d}z_i}F_{Z_i}(z_i, u_i=0\mid\lambda_0,\lambda_1) \\
= \frac{\text{d}}{\text{d}z_i} \frac{\lambda_1}{\lambda_0 + \lambda_1}e^{-z_i(\lambda_0 + \lambda_1)} - e^{\lambda_1z_i} \\ 
= \lambda_1 e^{-\lambda_1z_i}(1 - e^{-\lambda_0z_i}).
$$
Similar calculations follow for $u_i = 1,$ and we get 
$$
f_{Z_i}(z_i, u_i=1\mid \lambda_0, \lambda_1)  = \lambda_0e^{-\lambda_0z_i}(1 - e^{-\lambda_1z_i}).
$$
Combining the two cases, we get
$$
f_{Z_i, U_i}(z_i, u_i \mid \lambda_0, \lambda_1) = \begin{cases}
&\lambda_1e^{-\lambda_1z_i}(1 - e^{-\lambda_0z_i}),\quad \text{if }u_i=0\\
&\lambda_0e^{-\lambda_0z_i}(1 - e^{-\lambda_1z_i}), \quad \text{if }u_i = 1
\end{cases}.
$$
We then find the likelihood function of $\lambda_0,\lambda_1$ by 
$$
\mathcal{L}(\lambda_0,\lambda_1 \mid \mathbf{z}, \mathbf{u}) = f_{\mathbf{Z},\mathbf{U}}(\mathbf{z},\mathbf{u}\mid\lambda_0,\lambda_1) \\
= \prod_{i=1}^nf_{Z_i, U_i}(z_i, u_i \mid \lambda_0, \lambda_1) \\
= \prod_{i: u_i=0}\lambda_1e^{-\lambda_1z_i}(1 - e^{-\lambda_0z_i})\prod_{i:u_i=1}\lambda_0e^{-\lambda_0z_i}(1 - e^{-\lambda_1z_i}),
$$
with the corresponding log-likelihood function 
$$
\mathcal{l}(\lambda_0, \lambda_1 \mid \mathbf{z}, \mathbf{u}) = \sum_{i:u_i=0}\log(\lambda_1e^{-\lambda_1z_i}(1 - e^{-\lambda_0z_i})) + \sum_{i:u_i=1}\log(\lambda_0e^{-\lambda_0z_i}(1 - e^{-\lambda_1z_i})).
$$
To find the mle of $\lambda_0, \lambda_1$, we maximize $\mathcal{l}$ using the built-in optimizer in $\texttt{R}, \texttt{optim()}.$ Note that the $\texttt{optim}-$function finds the parameters that minimize the input function, so we use it to minimize $-\mathcal{l},$ which will give us the mle. 
```{r}
log.lik <- function(par, u, z){
  # par = c(lambda_0, lambda_1)
  u.0.idx <- which(u==0)
  u.1.idx <- which(u==1)
  result <- sum(log(par[2]*exp(-par[2]*z[u.0.idx])*(1 - exp(-par[1]*z[u.0.idx]))))
  result <- result + sum(log(par[1]*exp(-par[1]*z[u.1.idx])*(1 - exp(-par[2]*z[u.1.idx]))))
  return(-result)
}

mle <- optim(par=c(2,2), fn=log.lik, u=u, z=z)
mle
```
We get maximum likelihood estimators of $\hat{\lambda_0} = 3.46$ and $\hat{\lambda_1} = 9.33$, which are very close to the values we got for the estimators of $\lambda_0$ and $\lambda_1$ using the EM-method. An advantage of finding the mle optimizing the likelihood directly, if one can find an expression for the likelihood, is that the EM-algorithm might be very slow in convergence, and the performance might depend on the choice of initial values.